{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIS/oUN0KyHqvSQpupzT3o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/achika86/Machine-Learning/blob/main/VLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3_Gm_l5YWndO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import resnet50\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class HierarchicalVisionEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Hierarchical vision encoder that processes images at multiple scales\n",
        "    and adaptively selects the most relevant features.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 backbone='resnet50',\n",
        "                 scales=[224, 448, 896],\n",
        "                 feature_dim=768,\n",
        "                 num_patches=196,\n",
        "                 adaptive_pooling=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.scales = scales\n",
        "        self.feature_dim = feature_dim\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        # Shared backbone for all scales\n",
        "        if backbone == 'resnet50':\n",
        "            self.backbone = resnet50(pretrained=True)\n",
        "            # Remove final classification layers\n",
        "            self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])\n",
        "            backbone_dim = 2048\n",
        "\n",
        "        # Scale-specific adaptation layers\n",
        "        self.scale_adapters = nn.ModuleDict({\n",
        "            str(scale): nn.Sequential(\n",
        "                nn.Conv2d(backbone_dim, feature_dim, 1),\n",
        "                nn.LayerNorm([feature_dim]),\n",
        "                nn.GELU()\n",
        "            ) for scale in scales\n",
        "        })\n",
        "\n",
        "        # Attention mechanism for scale fusion\n",
        "        self.scale_attention = nn.MultiheadAttention(\n",
        "            embed_dim=feature_dim,\n",
        "            num_heads=12,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Adaptive pooling for consistent output size\n",
        "        if adaptive_pooling:\n",
        "            patch_size = int(math.sqrt(num_patches))\n",
        "            self.adaptive_pool = nn.AdaptiveAvgPool2d((patch_size, patch_size))\n",
        "        else:\n",
        "            self.adaptive_pool = None\n",
        "\n",
        "        # Position embeddings\n",
        "        self.pos_embedding = nn.Parameter(\n",
        "            torch.randn(1, num_patches, feature_dim) * 0.02\n",
        "        )\n",
        "\n",
        "    def encode_scale(self, x, scale):\n",
        "        \"\"\"Encode image at a specific scale.\"\"\"\n",
        "        # Resize input to target scale\n",
        "        x_scaled = F.interpolate(\n",
        "            x, size=(scale, scale),\n",
        "            mode='bilinear',\n",
        "            align_corners=False\n",
        "        )\n",
        "\n",
        "        # Extract features using backbone\n",
        "        features = self.backbone(x_scaled)\n",
        "\n",
        "        # Apply scale-specific adaptation\n",
        "        adapted_features = self.scale_adapters[str(scale)](features)\n",
        "\n",
        "        # Adaptive pooling if enabled\n",
        "        if self.adaptive_pool:\n",
        "            adapted_features = self.adaptive_pool(adapted_features)\n",
        "\n",
        "        # Flatten spatial dimensions\n",
        "        B, C, H, W = adapted_features.shape\n",
        "        adapted_features = adapted_features.view(B, C, H * W).transpose(1, 2)\n",
        "\n",
        "        return adapted_features\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Encode at each scale\n",
        "        scale_features = []\n",
        "        for scale in self.scales:\n",
        "            features = self.encode_scale(x, scale)\n",
        "            scale_features.append(features)\n",
        "\n",
        "        # Stack features from all scales\n",
        "        # Shape: [batch_size, num_scales * num_patches, feature_dim]\n",
        "        all_features = torch.cat(scale_features, dim=1)\n",
        "\n",
        "        # Apply cross-scale attention\n",
        "        attended_features, attention_weights = self.scale_attention(\n",
        "            all_features, all_features, all_features\n",
        "        )\n",
        "\n",
        "        # Pool to target number of patches\n",
        "        if attended_features.size(1) > self.num_patches:\n",
        "            # Use learned pooling or simple averaging\n",
        "            pooled_features = F.adaptive_avg_pool1d(\n",
        "                attended_features.transpose(1, 2),\n",
        "                self.num_patches\n",
        "            ).transpose(1, 2)\n",
        "        else:\n",
        "            pooled_features = attended_features\n",
        "\n",
        "        # Add position embeddings\n",
        "        pooled_features = pooled_features + self.pos_embedding\n",
        "\n",
        "        return pooled_features, attention_weights"
      ],
      "metadata": {
        "id": "i9Krq3KKW9zo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5174c152",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14b12111-944f-4bb9-f551-69f133776c72"
      },
      "source": [
        "# Assuming the model expects input images in the format [batch_size, channels, height, width]\n",
        "# Let's create a dummy batch of 4 images, each with 3 color channels (RGB) and a size of 224x224 pixels.\n",
        "batch_size = 4\n",
        "channels = 3\n",
        "height = 224\n",
        "width = 224\n",
        "\n",
        "sample_data = torch.randn(batch_size, channels, height, width)\n",
        "\n",
        "print(f\"Sample data tensor shape: {sample_data.shape}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample data tensor shape: torch.Size([4, 3, 224, 224])\n"
          ]
        }
      ]
    }
  ]
}